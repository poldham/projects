{"shortname": "benchmarks/ner_conll03", "title": "Named Entity Recognition (CoNLL-2003)", "description": ""}
{"shortname": "benchmarks/span-labeling-datasets", "title": "Span labeling datasets", "description": "This project compiles various NER and more general spancat datasets \nand their converters into the [spaCy format](https://spacy.io/api/data-formats). \nYou can use this to try out experiment with `ner` and `spancat`\nor to potentially pre-train them for your application.\n"}
{"shortname": "benchmarks/nel", "title": "NEL Benchmark", "description": "Pipeline for benchmarking NEL approaches (incl. candidate generation and entity disambiguation)."}
{"shortname": "benchmarks/textcat_architectures", "title": "Textcat performance benchmarks", "description": "Benchmarking different textcat architectures on different datasets."}
{"shortname": "benchmarks/ner_embeddings", "title": "Comparing embedding layers in spaCy", "description": "This project contains the code to reproduce the results of the\n[Multi hash embeddings in spaCy](https://arxiv.org/abs/2212.09255) technical report by Explosion.\nThe `project.yml` provides commands to download and preprocess the data sets as well as to\nrun the training and evaluation procedures. Different configuration of `vars` correspond\nto different experiments in the report.\n\nThere are a few scripts included that were used during the technical report writing process\nto run experiments in bulk and summarize the results. \nThe `scripts/run_experiments.py` runs multiple experiments one after the other\nby constructing and running `spacy project run` commands. The module\n`scipts/collate_results.py` summarizes the results of the same trials with multiple seeds.\nFinally, `scripts/plot_results.py` was used to produce the visualizations in the report.\nThese are all small command line apps and you can learn more about the usage as usual with the\n`--help` flag.\n\nThe `rows` argument for the `train-adjusted-rows` command is provided as a list and \nthis may lead to errors on Windows machines. Unfortunately, this might lead not being able to\nreproduce the `MultiHashEmbed (adjusted)` experiments from the paper on Windows using `run_experiment.py`.\nThis is due to known issue with handling quotes on Windows and is something we are looking into.\nThe config files can be edited by manually or in some other way to adjust the number of rows for \nthe hash embedding layers. We apologize for the inconvenience.\n"}
{"shortname": "benchmarks/ud_benchmark", "title": "Universal Dependencies v2.5 Benchmarks", "description": "This project template lets you train a spaCy pipeline on any [Universal Dependencies](https://universaldependencies.org/) corpus (v2.5) for benchmarking purposes. The pipeline includes an experimental trainable tokenizer, an experimental edit tree lemmatizer, and the standard spaCy tagger, morphologizer and dependency parser components. The CoNLL 2018 evaluation script is used to evaluate the pipeline. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `ud_treebank` and `spacy_lang` settings in the config. Use `xx` (multi-language) for `spacy_lang` if a particular language is not supported by spaCy. The tokenizer in particular is only intended for use in this generic benchmarking setup. It is not optimized for speed and it does not perform particularly well for languages without space-separated tokens. In production, custom rules for spaCy's rule-based tokenizer or a language-specific word segmenter such as jieba for Chinese or sudachipy for Japanese would be recommended instead."}
{"shortname": "benchmarks/healthsea_spancat", "title": "Healthsea-Spancat", "description": "This spaCy project uses the Healthsea dataset to compare the performance between the Spancat and NER architecture."}
{"shortname": "benchmarks/speed", "title": "Project for speed benchmarking of various pretrained models of different NLP libraries.", "description": "This project runs various models on unannotated text, to measure the average speed in words per second (WPS). Note that a fair comparison should also take into account the type of annotations produced by each model, and the accuracy scores of the various pretrained NLP tasks. This example project only addresses the speed issue, but can be extended to perform more detailed comparisons on any data."}
{"shortname": "benchmarks/parsing_penn_treebank", "title": "Dependency Parsing (Penn Treebank)", "description": ""}
{"shortname": "benchmarks/pretraining_morphologizer_oscar", "title": "Enhancing Morphological Analysis with spaCy Pretraining", "description": "This project explores the effectiveness of pretraining techniques on morphological analysis (morphologizer) by conducting experiments on multiple languages. The objective of this project is to demonstrate the benefits of pretraining word vectors using domain-specific data on the performance of the morphological analysis. We leverage the OSCAR dataset to pretrain our vectors for tok2vec and utilize the UD_Treebanks dataset to train a morphologizer component. We evaluate and compare the performance of different pretraining techniques and the performance of models without any pretraining."}
{"shortname": "nel/wikid", "title": "wikid", "description": "[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/32/main.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=32)\n[![spaCy](https://img.shields.io/static/v1?label=made%20with%20%E2%9D%A4%20and&message=spaCy&color=09a3d5&style=flat-square)](https://spacy.io)\n<br/>\n_No REST for the `wikid`_ :jack_o_lantern: - generate a SQLite database and a spaCy `KnowledgeBase` from Wikipedia & \nWikidata dumps. `wikid` was designed with the use case of named entity linking (NEL) with spaCy in mind.\n<br/>\nNote this repository is still in an experimental stage, so the public API might change at any time. \n"}
{"shortname": "ner_embeddings/spancat-datasets", "title": "Spancat datasets", "description": "This project compiles various NER and more general spancat datasets \nand their converters into the [spaCy format](https://spacy.io/api/data-formats). \nYou can use this to try out experiment with `ner` and `spancat`\nor to potentially pre-train them for your application.\n"}
{"shortname": "integrations/ray", "title": "Ray integration", "description": "Use [Ray](https://ray.io) and the [`spacy-ray`](https://github.com/explosion/spacy-ray) extension package for parallel and distributed training. To configure the number of workers, you can change the `n_workers` variable in the `project.yml`."}
{"shortname": "integrations/fastapi", "title": "FastAPI integration", "description": "Use [FastAPI](https://fastapi.tiangolo.com/) to serve your spaCy models and host modern REST APIs. To start the server, you can run `spacy project run start`. To explore the REST API interactively, navigate to `http://127.0.0.1:5000/docs` in your browser. See the examples for how to query the API using Python or JavaScript."}
{"shortname": "integrations/streamlit", "title": "Streamlit integration", "description": "[Streamlit](https://streamlit.io) is a Python framework for building interactive data apps. The [`spacy-streamlit`](https://github.com/explosion/spacy-streamlit) package helps you integrate spaCy visualizations into your Streamlit apps and quickly spin up demos to explore your pipelines interactively. It includes a full embedded visualizer, as well as individual components. If you're training your own pipelines, you can integrate the `visualize` command into your `project.yml` and pass in the path to your exported pipeline to visualize it. See the tutorial project templates for examples."}
{"shortname": "integrations/prodigy", "title": "Prodigy annotation tool integration", "description": "This project shows how to integrate the [Prodigy](https://prodi.gy) annotation tool (requires **v1.11+**) into your spaCy project template to automatically **export annotations** you've created and **train your model** on the collected data. Note that in order to run this template, you'll need to install Prodigy separately into your environment. For details on how the data was created, check out this [project template](https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands) and [blog post](https://explosion.ai/blog/sense2vec-reloaded#annotation).\n> \u26a0\ufe0f **Important note:** The example in this project uses a separate step `db-in` to export the example annotations into your database, so you can easily run it end-to-end. In your own workflows, you can leave this out and access the given dataset you've annotated directly."}
{"shortname": "integrations/wandb", "title": "Weights & Biases integration", "description": "Use [Weights & Biases](https://www.wandb.com/) for logging of training experiments. This project template uses the IMDB Movie Review Dataset and includes two workflows: `log` for training a simple text classification model and logging the results to Weights & Biases (works out-of-the-box and only requires the `[training.logger]` to be set in the config) and `parameter-search` for running a hyperparameter search using [Weights & Biases Sweeps](https://docs.wandb.ai/guides/sweeps), running the experiments and logging the results."}
{"shortname": "integrations/huggingface_hub", "title": "Hugging Face Hub integration", "description": "With [Hugging Face Hub](https://https://huggingface.co/), you can easily share any trained pipeline with the community. The Hugging Face Hub offers:\n\n- Free model hosting.\n- Built-in file versioning, even with very large files, thanks to a git-based approach.\n- In-browser widgets to play with the uploaded models.\n\nThis uses [`spacy-huggingface-hub`](https://github.com/explosion/spacy-huggingface-hub) to push a packaged pipeline to the Hugging Face Hub, including the `whl` file. This enables using `pip install`ing a pipeline directly from the Hugging Face Hub.\n"}
{"shortname": "experimental/ner_wikiner_speedster", "title": "Named Entity Recognition (WikiNER) accelerated using speedster", "description": "This project shows how `speedster` can accelerate spaCy's WikiNER pipeline.\n\n[Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) is an open-source tool designed to accelerate AI inference of deep learning models in a few lines of code. Within the WikiNER pipeline, `speedster` optimizes BERT to achieve the maximum acceleration physically possible on the hardware used.\n\n`Speedster` is built on top of [Nebullvm](https://github.com/nebuly-ai/nebullvm), an open-source framework for building AI-optimization tools.\n\nFurther info on the WikiNER pipeline can be found in [this section](https://github.com/explosion/projects/tree/v3/pipelines/ner_wikiner)."}
{"shortname": "experimental/ner_spancat_compare", "title": "Comparing SpanCat and NER using a corpus of biomedical literature (GENIA)", "description": "This project demonstrates how spaCy's Span Categorization (SpanCat) and\nNamed-Entity Recognition (NER) perform on different types of entities. Here, we used\na dataset of biomedical literature containing both overlapping and non-overlapping spans.\n"}
{"shortname": "experimental/coref", "title": "Training a spaCy Coref Model", "description": "This project trains a coreference model for spaCy using OntoNotes.\n"}
{"shortname": "experimental/ner_spancat", "title": "Example SpanCategorizer project using Indonesian NER", "description": "The SpanCategorizer is a component in **spaCy v3.1+** for assigning labels to contiguous spans of text proposed by a customizable suggester function. Unlike spaCy's EntityRecognizer component, the SpanCategorizer can recognize nested or overlapping spans. It also doesn't rely as heavily on consistent starting and ending words, so it may be a better fit for non-NER span labelling tasks. You do have to write a function that proposes your candidate spans, however. If your spans are often short, you could propose all spans under a certain size. You could also use syntactic constituents such as noun phrases or noun chunks, or matcher rules."}
{"shortname": "pipelines/tagger_parser_ud", "title": "Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)", "description": "This project template lets you train a part-of-speech tagger, morphologizer, lemmatizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\n"}
{"shortname": "pipelines/floret_wiki_oscar_vectors", "title": "Train floret vectors from Wikipedia and OSCAR", "description": "This project downloads, extracts and preprocesses texts from Wikipedia and\nOSCAR and trains vectors with [floret](https://github.com/explosion/floret).\n\nBy default, the project trains floret vectors for Macedonian.\n"}
{"shortname": "pipelines/textcat_multilabel_demo", "title": "Demo Multilabel Textcat (Text Classification)", "description": "A minimal demo textcat_multilabel project for spaCy v3."}
{"shortname": "pipelines/floret_ko_ud_demo", "title": "Demo floret vectors for UD Korean Kaist", "description": "Train floret vectors on OSCAR and compare no vectors, standard vectors, and floret vectors on UD Korean Kaist."}
{"shortname": "pipelines/textcat_demo", "title": "Demo Textcat (Text Classification)", "description": "A minimal demo textcat project for spaCy v3. The demo data comes from the [tutorials/textcat_docs_issues](https://github.com/explosion/projects/tree/v3/tutorials/textcat_docs_issues) project."}
{"shortname": "pipelines/ner_demo_replace", "title": "Demo replacing an NER component in a pretrained pipeline", "description": "A minimal demo NER project that replaces the NER component in an existing pretrained pipeline. All other pipeline components are preserved and frozen during training."}
{"shortname": "pipelines/tagger_parser_predicted_annotations", "title": "Using Predicted Annotations in Subsequent Components", "description": "This project shows how to use the predictions from one pipeline component as features for a subsequent pipeline component in **spaCy v3.1+**. In this demo, which trains a parser and a tagger on [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT), the `token.dep` attribute from the parser is used as a feature by the tagger. To make the predicted `DEP` available to the tagger during training, `DEP` is added to `[components.tagger.model.tok2vec.embed.attrs]` and `parser` is added to `[training.annotating_components]` in the config. This particular example does not lead to a large difference in performance, but the tagger accuracy improves from to 92.67% to 92.97% with the addition of `DEP`."}
{"shortname": "pipelines/spancat_demo", "title": "Demo spancat in a new pipeline (Span Categorization)", "description": "A minimal demo spancat project for spaCy v3"}
{"shortname": "pipelines/ner_wikiner", "title": "Named Entity Recognition (WikiNER)", "description": "Simple example of downloading and converting source data and training a named entity recognition model. The example uses the WikiNER corpus, which was constructed semi-automatically. The main advantage of this corpus is that it's freely available, so the data can be downloaded as a project asset. The WikiNER corpus is distributed in IOB format, a fairly common text encoding for sequence data. The `corpus` subcommand splits the corpus into training, development and testing partitions, and uses `spacy convert` to convert them into spaCy's binary format. You can then edit the config to try out different settings, and trigger training with the `train` subcommand."}
{"shortname": "pipelines/parser_intent_demo", "title": "Demo Intent Parser (Dependency Parser)", "description": "A minimal demo parser project for spaCy v3 adapted from the spaCy v2 [`train_intent_parser.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_intent_parser.py) example script."}
{"shortname": "pipelines/parser_demo", "title": "Demo Dependency Parser", "description": "A minimal demo parser project for spaCy v3 adapted from the spaCy v2 [`train_parser.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_parser.py) example script."}
{"shortname": "pipelines/floret_vectors_demo", "title": "Demo floret vectors", "description": "Train floret vectors and load them into a spaCy vectors model."}
{"shortname": "pipelines/ner_demo_update", "title": "Demo updating an NER component in a pretrained pipeline", "description": "A demo NER project that updates the NER component in an existing pretrained pipeline. All other pipeline components are preserved and frozen during training."}
{"shortname": "pipelines/polar_component", "title": "Polar Component", "description": "This example project shows how to implement a simple stateful component to\nscore docs on semantic poles.\n\nThe method here is based on SemAxis from [An et al\n2018](https://arxiv.org/abs/1806.05521). The basic idea is that given a set\nof word vectors and some seed poles, like \"bad-good\", it's possible to\ncalculate reference vectors. The distance of document vectors from those\nreference vectors is like a sentiment or polar score of the document. While\nnot as sophisticated as a trained model, it's easy to test with existing data.\n\nIf you use enough poles, you can use the scores as semantic vectors that can\nmake downstream tasks explainable. This is explored in the SemAxis paper as\nwell as [Mathew et al 2020](https://arxiv.org/abs/2001.09876), \"The Polar\nFramework\". (Incorporating semantic vectors as features in a spaCy model is\nleft as an exercise for the reader.) \n\n**Note:** Because the data is hosted on Kaggle, it can't be automatically\ndownloaded by `spacy project assets`, so you'll have to download it yourself.\nSee [the assets section of this README](#assets) for the link.\n"}
{"shortname": "pipelines/edit_tree_lemmatizer", "title": "Demo the trainable edit-tree lemmatizer", "description": ""}
{"shortname": "pipelines/ner_demo", "title": "Demo NER in a new pipeline (Named Entity Recognition)", "description": "A minimal demo NER project for spaCy v3 adapted from the spaCy v2 [`train_ner.py`](https://github.com/explosion/spaCy/blob/v2.3.x/examples/training/train_ner.py) example script for creating an NER component in a new pipeline."}
{"shortname": "pipelines/floret_fi_core_demo", "title": "Demo floret vectors for Finnish", "description": "Train floret vectors on OSCAR and compare standard vectors vs. floret vectors on UD Finnish TDT and turku-ner-corpus."}
{"shortname": "floret_wiki_oscar_vectors/vectors", "title": "Train floret vectors from Wikipedia and OSCAR", "description": "This project downloads, extracts and preprocesses texts from Wikipedia and\nOSCAR and trains vectors with [floret](https://github.com/explosion/floret).\n\nBy default, the project trains floret vectors for Macedonian.\n\nPrerequisites:\n- a large amount of hard drive space\n- a workstation with a good CPU, or a lot of patience\n\nFor Macedonian, you'll need ~5GB in `/scratch` and ~1GB in `vectors/`.\n\nAdjust the variables `n_process` and `vector_thread` for your CPU.\n\n## Text Sources\n\n- Wikipedia: https://dumps.wikimedia.org\n- OSCAR 2019: https://oscar-corpus.com/post/oscar-2019/\n\nBy default the full OSCAR 2019 dataset is loaded in streaming mode. Adjust\n`oscar_max_texts` to use a subset of the full dataset, especially for large\nlanguages like English, Spanish, Chinese, Russian, etc.\n\n## floret Parameters\n\n[floret](https://github.com/explosion/floret) has a large number of\nparameters and it's difficult to give advice for all configurations, but the\nparameters described here are the ones that it makes sense to customize for\nany new language and to experiment with initially.\n\nBe aware that if you're using more than one thread, the results of each run\nwith fastText or floret will be slightly different.\n\n### `vector_minn` / `vector_maxn`\n\nThe minimum and maximum character n-gram lengths should be adapted for the\nlanguage and writing system. The n-grams should capture common grammatical\naffixes like English `-ing`, without making the number of n-grams per word\ntoo large. Very short n-grams aren't meaningful and very long n-grams will be\ntoo sparse and won't be useful for cases with misspellings and noise.\n\nA good rule of thumb is that `maxn` should correspond to the length of the\nlongest common affix + `1`, so for many languages with alphabets, `minn\n5`/`maxn 5` can be a good starting point, similar to the defaults in the\n[original fastText vectors](https://fasttext.cc/docs/en/crawl-vectors.html).\n\nFor writing systems where one character corresponds to a syllable, shorter\nn-grams are typically more suitable. For Korean, where each (normalized)\ncharacter is a syllable and most grammatical affixes are 1-2 characters,\n`minn 2`/`maxn 3` seems to perform well.\n\n### `vector_bucket`\n\nThe bucket size is the number of rows in the floret vector table. For\ntagging and parsing, a bucket size of 50k performs well, but larger sizes may\nstill lead to small improvements. For NER, the performance continues to\nimprove for bucket sizes up to at least 200k.\n\nIn a spaCy pipeline package, 50k 300-dim vectors are ~60MB and 200k 300-dim\nvectors are ~230MB.\n\n### `vector_hash_count`\n\nThe recommended hash count is `2`, especially for smaller bucket sizes.\n\nLarger hash counts are slower to train with floret and slightly slower in\ninference in spaCy, but may lead to slightly improved performance, especially\nwith larger bucket sizes.\n\n### `vector_epoch`\n\nYou may want to reduce the number of epochs for larger training input sizes.\n\n### `vector_min_count`\n\nYou may want to increase the minimum word count for larger training input\nsizes.\n\n### `vector_lr`\n\nYou may need to decrease the learning rate for larger training input sizes to\navoid NaN errors, see:\nhttps://fasttext.cc/docs/en/faqs.html#im-encountering-a-nan-why-could-this-be\n\n### `vector_thread`\n\nAdjust the number of threads for your CPU. With a larger number of threads,\nyou may need more epochs to reach the same performance.\n\n## Notes\n\nThe project does not currently clean up any intermediate files so that it's\npossible to resume from any point in the workflow. The overall disk space\ncould be reduced by cleaning up files after each step, keeping only the final\nfloret input text file. floret does require the input file to be on disk\nduring training.\n\nfloret always writes the full `.bin` and `.vec` files after training. These\nmay be 5GB+ each even though the final `.floret` table is much smaller.\n\nImport the floret vectors into a spaCy vectors model with:\n\n```shell\nspacy init vectors mk vectors/mk.floret /path/to/mk_vectors_model --mode floret\n```\n"}
{"shortname": "tutorials/ner_multiple_trials", "title": "Training a named-entity recognition (NER) with multiple trials", "description": "This project demonstrates how to train a spaCy pipeline with multiple trials.\nIt trains a named-entity recognition (NER) model on the WikiNEuRal English\ndataset.  Having multiple trials is useful for experiments, especially if we\nwant to account for variance and *dependency* on a random seed. \n\nUnder the hood, the training script in `scripts/train_with_trials.py`\ngenerates a random seed per trial, and runs the `train` command as usual.  You\ncan find the trained model per trial in `training/trial_{n}/`.\n\n> **Note**\n> Because the WikiNEuRal dataset is large, we're limiting the number of samples in the train\n> and dev corpus to 500 for demonstration purposes. You can adjust this by\n> overriding `vars.limit_samples`, or setting it to `0` to train on the whole\n> training corpus.\n\nAt evaluation, you can pass a directory containing all the models for each\ntrial. This process is demonstrated in `scripts/evaluate_with_trials.py`.\nThis will then result to multiple `metrics/scores.json` files that you can\nsummarize.\n"}
{"shortname": "tutorials/ner_fashion_brands", "title": "Detecting fashion brands in online comments (Named Entity Recognition)", "description": "This project uses [`sense2vec`](https://github.com/explosion/sense2vec) and [Prodigy](https://prodi.gy) to bootstrap an NER model to detect fashion brands in [Reddit comments](https://files.pushshift.io/reddit/comments/). For more details, see [our blog post](https://explosion.ai/blog/sense2vec-reloaded#annotation)."}
{"shortname": "tutorials/nel_emerson", "title": "Disambiguation of \"Emerson\" mentions in sentences (Entity Linking)", "description": "**This project was created as part of a [step-by-step video tutorial](https://www.youtube.com/watch?v=8u57WSXVpmw).** It uses [spaCy](https://spacy.io)'s entity linking functionality and [Prodigy](https://prodi.gy) to disambiguate \"Emerson\" mentions in text to unique identifiers from Wikidata. As an example use-case, we consider three different people called Emerson: [an Australian tennis player](https://www.wikidata.org/wiki/Q312545), [an American writer](https://www.wikidata.org/wiki/Q48226), and a [Brazilian footballer](https://www.wikidata.org/wiki/Q215952). [See here](https://github.com/explosion/projects/tree/master/nel-emerson) for the previous scripts for spaCy v2.x."}
{"shortname": "tutorials/spanruler_restaurant_reviews", "title": "Using SpanRuler for rule-based Named Entity Recognition", "description": "This example project demonstrates how you can use the\n[SpanRuler](https://spacy.io/api/spanruler), a component introduced in spaCy\n3.3, for rule-based named entity recognition (NER). In spaCy v3 and below,\nthis functionality can be achieved via the\n[EntityRuler](https://spacy.io/api/entityruler). However, we will start\n**deprecating** the `entity_ruler` component in favor of `span_ruler` in v4.\n\nHere, we will be using the **MIT Restaurant dataset** (Liu, et al, 2013) to\ndetermine entities such as *Rating*, *Location*, *Restaurant_Name*,\n*Price*, *Dish*, *Amenity*,  and *Cuisine* from restaurant reviews.\nBelow are a few examples from the training data:\n\n![](figures/example_00.png)\n![](figures/example_01.png)\n![](figures/example_02.png)\n\nFirst, we will train an NER-only model and treat it as our baseline. Then, we\nwill attach the `SpanRuler` component **after the `ner` component** of the\nexisting pipeline. This setup gives us two pipelines we can compare upon. The\nrules for each entity type can be found in the `scripts/rules.py` file.\n\nIf we look at the results, we see an increase in performance for the majority\nof entities with rules:\n\n|          | NER only  | With Spanruler  |\n|----------|-----------|-----------------|\n| Price    | 81.68     | **83.23**       |\n| Rating   | 78.42     |   78.06         |\n| Hours    | 64.91     | **65.80**       |\n| Amenity  | 64.26     | **64.96**       |\n| Location | 82.28     | **82.82**       |\n| Restaurant_Name| 76.88     | **78.92**       |\n\nOverall, we have better performance for the combined `ner` and `span_ruler`\npipeline with our set of rules.\n\n|           | NER only | With Spanruler |\n|-----------|----------|----------------|\n| Precision | 76.39    | **77.06**      |\n| Recall    | 76.64    | **77.40**      |\n| F-score   | 76.52    | **77.23**      |\n\nWhen we noticed some inconsistencies in the original dataset, we went back and\nfixed them with a [Prodigy](https://prodi.gy) workflow. The commands are included\nhere to reproduce our process for annotation, but we've also included the outputted\ndatasets so you can directly skip to training a new model. \n\nWith the new annotations and rules, we saw an improvement in both the NER and NER with \nSpanruler pipelines.\n\n|          | NER only  | With Spanruler  |\n|----------|-----------|-----------------|\n| Price    | 87.00     | **87.25**       |\n| Rating   | 89.39     | **92.55**       |\n| Hours    | 82.12     | **82.52**       |\n| Amenity  | 80.95     | **83.07**       |\n| Location | 92.03     | **92.70**       |\n| Restaurant_Name| 82.90     | **87.48**       |\n| Cuisine  | 90.00     | **91.09**       |\n| Dish     | 83.05     | **85.66**       |\n\nOverall, we have better performance for the combined `ner` and `span_ruler`\npipeline with our new set of rules.\n\n|           | NER only | With Spanruler |\n|-----------|----------|----------------|\n| Precision | 87.05    | **88.86**      |\n| Recall    | 86.31    | **88.10**      |\n| F-score   | 86.68    | **88.48**      |\n\n**Reference**\n\n- J. Liu, P. Pasupat, S. Cyphers, and J. Glass. 2013. Asgard: A portable\narchitecture for multilingual dialogue systems. In *2013 IEEE International\nConference on Acoustics, Speech and Signal Processing*, pages 8386-8390\n"}
{"shortname": "tutorials/parser_low_resource", "title": "Training a POS tagger and dependency parser for a low-resource language", "description": "This project trains a part-of-speech tagger and dependency parser for a\nlow-resource language such as Tagalog. We will be using the\n[TRG](https://universaldependencies.org/treebanks/tl_trg/index.html) and\n[Ugnayan](https://universaldependencies.org/treebanks/tl_ugnayan/index.html)\ntreebanks for this task. Since the number of sentences in each corpus is\nsmall, we'll need to evaluate our model using [10-fold cross\nvalidation](https://universaldependencies.org/release_checklist.html#data-split).\nHow to implement this split will be demonstrated in this project\n(`scripts/kfold.py`). The cross validation results can be seen below.\n\n### 10-fold Cross-validation results\n\n|         | TOKEN_ACC | POS_ACC | MORPH_ACC | TAG_ACC | DEP_UAS | DEP_LAS |\n|---------|-----------|---------|-----------|---------|---------|---------|\n| TRG     | **1.000**     | **0.843**   | 0.749     | **0.833**   | *80.846**   | **0.554**   |\n| Ugnayan | 0.998     | 0.819   | **0.995**     | 0.810   | 0.667   | 0.409   |\n"}
{"shortname": "tutorials/ner_double", "title": "Combining Multiple Trained NER Components", "description": "This project shows you the different ways you can combine multiple trained NER components and their tradeoffs.\n"}
{"shortname": "tutorials/spancat_food_ingredients", "title": "Span Categorization in Prodigy", "description": "This project shows how to use Prodigy to annotate data for the spancat component"}
{"shortname": "tutorials/ner_drugs", "title": "Detecting drug names in online comments (Named Entity Recognition)", "description": "This project uses [Prodigy](https://prodi.gy) to bootstrap an NER model to detect drug names in [Reddit comments](https://files.pushshift.io/reddit/comments/)."}
{"shortname": "tutorials/ner_tweets", "title": "Detecting people entities in tweets (Named Entity Recognition)", "description": "This project demonstrates how to improve spaCy's pretrained models by\naugmenting the training data and adapting it to a different domain.\n"}
{"shortname": "tutorials/ner_food_ingredients", "title": "Analyzing how mentions of ingredients change over time (Named Entity Recognition)", "description": "**This project was created as part of a [step-by-step video tutorial](https://www.youtube.com/watch?v=59BKHO_xBPA).** It uses [`sense2vec`](https://github.com/explosion/sense2vec) and [Prodigy](https://prodi.gy) to bootstrap an NER model to detect ingredients [Reddit comments](https://files.pushshift.io/reddit/comments/) and to calculate how mentions change over time. The results were then used to create a [bar chart race visualization](https://public.flourish.studio/visualisation/1532208/) of selected ingredients."}
{"shortname": "tutorials/rel_component", "title": "Example project of creating a novel nlp component to do relation extraction from scratch.", "description": "This example project shows how to implement a spaCy component with a custom Machine Learning model, how to train it with and without a transformer, and how to apply it on an evaluation dataset."}
{"shortname": "tutorials/textcat_docs_issues", "title": "Predicting whether a GitHub issue is about documentation (Text Classification)", "description": "This project uses [spaCy](https://spacy.io) with annotated data from [Prodigy](https://prodi.gy) to train a **binary text classifier** to predict whether a GitHub issue title is about documentation. The pipeline uses the component `textcat_multilabel` in order to train a binary classifier using only one label, which can be True or False for each document. An equivalent alternative for a binary text classifier would be to use the `textcat` component with two labels, where exactly one of the two labels is True for each document."}
{"shortname": "tutorials/ner_pytorch_medical", "title": "Detecting entities in Medical Records with PyTorch", "description": "This project uses the [i2b2 (n2c2) 2011 Challenge Dataset](https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/) to bootstrap a PyTorch NER model to detect entities in Medical Records. It also demonstrates how to anonymize medical records for annotators in [Prodigy](https://prodi.gy)."}
{"shortname": "tutorials/textcat_goemotions", "title": "Categorization of emotions in Reddit posts (Text Classification)", "description": "This project uses spaCy to train a text classifier on the [GoEmotions dataset](https://github.com/google-research/google-research/tree/master/goemotions) with options for a pipeline with and without transformer weights. To use the BERT-based config, change the `config` variable in the `project.yml`. \n\n> The goal of this project is to show how to train a spaCy classifier based on a csv file, not to showcase a model that's ready for production. The GoEmotions dataset has known flaws described [here](https://github.com/google-research/google-research/tree/master/goemotions#disclaimer) as well as label errors resulting from [annotator disagreement](https://www.youtube.com/watch?v=khZ5-AN-n2Y).\n"}
